"""
Results Analysis and Visualization Module
=========================================

This file provides utility functions for loading, evaluating, and visualizing
Reinforcement Learning experiment results, such as comparing performance across
different hyperparameter configurations (e.g., multiple values of gamma, alpha,
or epsilon). It is designed to work with log data generated by TensorBoard or
custom log files during training.

# Main responsibilities:
- load training metrics such as episode rewards from multiple experimental runs,
- compute evaluation statistics: mean performance, variance/stability, and learning progression,
- generate comparative plots (mean Â± standard deviation) for hyperparameter sweeps,
- export analysis artifacts (graphs, numeric tables) for publication and documentation.

# Dependencies:
- numpy (data manipulation and statistics)
- matplotlib (visualization)
- compatible with Gymnasium-based Q-Learning training results

Author: Calin Dragos George
Created: 26 October 2025
"""
import os
import numpy as np
import matplotlib.pyplot as plt

from tensorboard.backend.event_processing import event_accumulator


def load_episode_rewards(log_dir):
    """
    load episode rewards from a tensorboard log directory.
    returns a list of episode reward values.

    log_dir: path to the tensorBoard run folder, e.g.: "logs/qlearning_cartpole"
    """
    if not os.path.exists(log_dir):
        raise FileNotFoundError(f"Log directory does not exist: {log_dir}")

    # load tensorboard events
    ea = event_accumulator.EventAccumulator(log_dir)
    ea.Reload()

    # extract scalar data (episode reward)
    if "Episode Reward" not in ea.Tags().get("scalars", []):
        raise KeyError("No 'Episode Reward' data found in TensorBoard logs.")

    events = ea.Scalars("Episode Reward")
    rewards = [event.value for event in events]

    return rewards


def plot_rewards(rewards, title="Training Progress: Episode Reward"):
    """
    plot episode reward progression across training.
    helps visually inspect stability and improvement.
    """
    episodes = np.arange(len(rewards))

    plt.figure(figsize=(10, 5))
    plt.plot(episodes, rewards, label="Episode Reward", linewidth=1.2)
    plt.xlabel("Episodes")
    plt.ylabel("Reward")
    plt.title(title)
    plt.grid(True)
    plt.tight_layout()
    plt.show()


def analyze_logs(log_dir):
    """
    main helper function:
    - loads rewards from tensorboard logs
    - generates a simple reward plot
    """
    rewards = load_episode_rewards(log_dir)
    plot_rewards(rewards)

if __name__ == "__main__":
    analyze_logs("logs/qlearning_cartpole")
